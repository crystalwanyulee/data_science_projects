{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.linalg as scipy_linalg\n",
    "from collections import defaultdict \n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "#os.chdir('C://Users/wanyu/Documents/Computational Linguilistics/PA5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSWC():\n",
    "    \n",
    "    def __init__(self, word_dict=None, ppmi_matrix=None):\n",
    "        self.word_dict = word_dict\n",
    "        self.freq_matrix = None\n",
    "        self.ppmi_matrix = ppmi_matrix\n",
    "        self.V = None    \n",
    "\n",
    "        \n",
    "    def PPMI(self, sentences):\n",
    "        \n",
    "        # Create a word dictionary\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(sentence)\n",
    "        \n",
    "        words = list(set(words))\n",
    "        self.word_dict = {word: index for index, word in enumerate(words)}\n",
    "        \n",
    "        # Creating a co-occurence matrix\n",
    "        self.freq_matrix = np.zeros((len(words), len(words)))\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            for index, word in enumerate(sentence):\n",
    "                if index != 0: \n",
    "                    context_idx = self.word_dict[sentence[index-1]]\n",
    "                    word_idx = self.word_dict[word]\n",
    "                    self.freq_matrix[word_idx, context_idx] += 1\n",
    "\n",
    "        # Repeating sentnences and Smoothing counts            \n",
    "        self.freq_matrix = self.freq_matrix*10 + 1\n",
    "\n",
    "        total = self.freq_matrix.sum().sum()\n",
    "        \n",
    "        # Computing PPMI\n",
    "        join_prob = self.freq_matrix/total\n",
    "        word_prob = self.freq_matrix.sum(axis=1)/total\n",
    "        context_prob = self.freq_matrix.sum(axis=0)/total\n",
    "        \n",
    "        independent_prob = (word_prob[:,None]*context_prob).T\n",
    "        self.ppmi_matrix = np.log(join_prob*(1/independent_prob))\n",
    "        \n",
    "        self.ppmi_matrix = np.maximum(self.ppmi_matrix, np.zeros((len(self.word_dict), len(self.word_dict))))\n",
    "\n",
    "    def SVD(self):\n",
    "        U, E, Vt = scipy_linalg.svd(self.ppmi_matrix, full_matrices=False)\n",
    "        U = np.matrix(U) # compute U\n",
    "        E = np.matrix(np.diag(E)) # compute E\n",
    "        Vt = np.matrix(Vt) # compute Vt = conjugage transpose of V\n",
    "        \n",
    "        return Vt.T # compute V = conjugate transpose of Vt\n",
    "    \n",
    "    def reduced_PPMI(self, dimension):\n",
    "        self.V = self.SVD()\n",
    "        reduced_PPMI = self.ppmi_matrix * self.V[:, 0:dimension]\n",
    "        \n",
    "        return reduced_PPMI\n",
    "    \n",
    "    def word_vector(self, words, word_matrix=None):\n",
    "        '''\n",
    "        Get a word vector based on the default matrix\n",
    "        '''\n",
    "        \n",
    "        if word_matrix is None:\n",
    "            word_matrix = self.ppmi_matrix                  \n",
    "        \n",
    "        if type(words) == list:\n",
    "            word_index = [self.word_dict[word] for word in words]\n",
    "            word_vector = word_matrix[word_index].astype('float')\n",
    "            \n",
    "        else: \n",
    "             word_index = self.word_dict[words]\n",
    "        \n",
    "        word_vector = word_matrix[word_index]\n",
    "        \n",
    "        return word_vector\n",
    "    \n",
    "    \n",
    "    def euclidean(self, word1, word2, word_matrix=None, v_input=False):\n",
    "        \n",
    "        '''\n",
    "        Compute the Euclidean distance between two vectors\n",
    "        '''\n",
    "        \n",
    "        if word_matrix is None:\n",
    "            word_matrix = self.ppmi_matrix   \n",
    "        \n",
    "        if v_input:\n",
    "            v1, v2 = word1, word2\n",
    "        \n",
    "        else:\n",
    "            v1 = self.word_vector(word1, word_matrix)\n",
    "            v2 = self.word_vector(word2, word_matrix)\n",
    "        \n",
    "        distance = scipy_linalg.norm(v2-v1)\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def cosine_similarity(self, word1, word2, word_matrix=None, v_input=False):\n",
    "        '''\n",
    "        Compute the cosine similarity between two words or one word and several words\n",
    "        '''\n",
    "        \n",
    "        if word_matrix is None:\n",
    "            word_matrix = self.ppmi_matrix   \n",
    "        \n",
    "        if v_input:\n",
    "            v1, v2 = word1, word2\n",
    "        \n",
    "        else:\n",
    "            v1 = self.word_vector(word1, word_matrix)\n",
    "            v2 = self.word_vector(word2, word_matrix)\n",
    "            \n",
    "        length_v1 = scipy_linalg.norm(v1)\n",
    "        length_v2 = scipy_linalg.norm(v2, axis=1)\n",
    "        denominator = length_v1 * length_v2\n",
    "        numerator = np.squeeze(np.asarray(v1.dot(v2.T)))\n",
    "        similarity = numerator*(1/denominator)\n",
    "        \n",
    "        return similarity  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create distributional semantic word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files\n",
    "with open('dist_sim_data.txt') as f:\n",
    "    sentences = f.read().splitlines()\n",
    "    sentences = [sentence.split() for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Compare the word vector for \"dogs\" before and after PPMI reweighting. Does PPMI do the right thing to the count matrix? Why? Explain in a few sentences how PPMI helps**\n",
    "\n",
    "Yes, PPMI indeed improves the measurement of the association between words. Instead of directly using raw counts, it captures how much more the two words co-occur than two words appear by chance. Therefore, it can discriminate against essential words.\n",
    "\n",
    "As we can see in the following table, in the raw count vector, the context word, \"the\", has the highest frequency, and its value is also other than 1 compared to other context words. The PPMI vector has similar results, and only \"the\" has a non-zero value. However, the range between \"the\" and other context words become smaller in the PPMI vector. Therefore, PPMI can not only correctly discriminate important words but also mitigate a skewed problem of raw counts. Because \"the\" are ubiquitous in all sentences, the prior\n",
    "probability of \"the\" will be high, and the independent probability of \"dogs\" and \"the\" will also be high. Thus, the importance of \"the\" will significantly decrease. Although \"the\" is not the right word to differentiate \"dogs\" with other nouns, it can help us to discriminate it with other parts-of-speech, such as verbs. That's the reason why \"the\" still has a certain amount of PPMI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>men</th>\n",
       "      <th>like</th>\n",
       "      <th>dogs</th>\n",
       "      <th>feed</th>\n",
       "      <th>bite</th>\n",
       "      <th>the</th>\n",
       "      <th>women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Raw Counts</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>91.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPMI</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            men  like  dogs  feed  bite    the  women\n",
       "Raw Counts  1.0   1.0   1.0   1.0   1.0  91.00    1.0\n",
       "PPMI        0.0   0.0   0.0   0.0   0.0   2.09    0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dswc = DSWC()        \n",
    "dswc.PPMI(sentences)\n",
    "\n",
    "dogs = pd.DataFrame({'Raw Counts': dswc.word_vector('dogs', dswc.freq_matrix).ravel(), \n",
    "                   'PPMI': dswc.word_vector('dogs').ravel()}, index=list(dswc.word_dict.keys()))\n",
    "dogs.T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2: Do the distances you compute above confirm our intuition from distributional semantics (i.e. similar words appear in similar contexts)?**\n",
    "\n",
    "Yes, the distances reflect the fact that similar words appear in similar contexts.The first three pairs are all nouns and have similar distances because their contexts are the same word, \"the\". Although \"men\" seems more closer to \"dogs\" than to \"women\", it might result from the difference of the total frequency in our dataset. If we compute how many times those nouns are shown in our dataset, we can discover that \"dogs\" appears 91 times, \"men\" occurs 81 times and \"women\" only shows 51 times. Thus, we can conclude that the differences of distance here between nouns are determined by how often a word appears in all sentences.\n",
    "Although human-related and animal-related nouns don't have a significant difference, we can see the apparent disparity between human-related and animal-related verbs. Since both \"feed\" and \"like\" belong to human-related verbs, they should be close to each other but be far from the animal-related verb. This fact is revealed by the above measurement of distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compact PPMI</th>\n",
       "      <th>reduced PPMI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pairs</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>women_men</th>\n",
       "      <td>0.2234</td>\n",
       "      <td>0.2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>women_dogs</th>\n",
       "      <td>0.3398</td>\n",
       "      <td>0.3398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>men_dogs</th>\n",
       "      <td>0.1164</td>\n",
       "      <td>0.1164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feed_like</th>\n",
       "      <td>0.6674</td>\n",
       "      <td>0.5220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feed_bite</th>\n",
       "      <td>2.1746</td>\n",
       "      <td>2.1701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like_bite</th>\n",
       "      <td>1.7205</td>\n",
       "      <td>1.6985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            compact PPMI  reduced PPMI\n",
       "pairs                                 \n",
       "women_men         0.2234        0.2234\n",
       "women_dogs        0.3398        0.3398\n",
       "men_dogs          0.1164        0.1164\n",
       "feed_like         0.6674        0.5220\n",
       "feed_bite         2.1746        2.1701\n",
       "like_bite         1.7205        1.6985"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_original,sim_reduced = [], []\n",
    "pairs = [['women','men'],['women','dogs'],['men','dogs'],\n",
    "         ['feed','like'],['feed','bite'], ['like','bite']]\n",
    "reduced_PPMI = dswc.reduced_PPMI(3)\n",
    "\n",
    "for pair in pairs:\n",
    "    original = round(float(dswc.euclidean(pair[0], pair[1])),4)\n",
    "    reduced = round(float(dswc.euclidean(pair[0], pair[1], reduced_PPMI)),4)\n",
    "    sim_original.append(original)\n",
    "    sim_reduced.append(reduced)\n",
    "    \n",
    "pairs_text = ['_'.join(pair) for pair in pairs]\n",
    "distance = pd.DataFrame({'pairs':pairs_text, 'compact PPMI': sim_original,\n",
    "                          'reduced PPMI': sim_reduced})\n",
    "\n",
    "distance.set_index('pairs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3: Does the compact/reduced matrix still keep the information we need for each word vector?**\n",
    "\n",
    "Overall, the reduced PPMI remains most information for each word vector(the above table).\n",
    "Although only the distance between \"feed\" and \"like\" decreases slightly, it still reflects\n",
    "that \"feed\" and \"like\" are closer to each other and don't influence the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Synonym Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Objective:** Use two types of word vectors to conduct synonym detection.\n",
    "\n",
    "* **Description of Word Vectors:**\n",
    "  1. **Classic distributional semantic matrix**: The matrix is trained using a 2-word context window, PPMI weighting, and SVD reduction to 500 dimensions. The co-occurrence statistics are computed from the British National Corpus and the Web-As-Corpus. It is trained by the COMPOSES toolkit. You could in fact make your own using the pipeline from part I, but SVD takes hours in a realistic setting|cubic time (O(n3)) in the number of dimensions.\n",
    "  2. **Google’s 300-dimensional word vectors** trained by deep learning. The version we provide is trained on the Google News corpus and taken from the word2vec toolkit. We will see how their word vectors fare against the more classical method.\n",
    "\n",
    "* **Tasks:**  \n",
    "  1. Create a multiple-choice question test set: For each verb, you will pick 1 of the synonyms and 4 random non-synonyms from the data set. Make 1,000 such synonym multiple-choice questions. \n",
    "  2. Use the word vectors and to calculate **Euclidean distance** and **cosine similarity** to pick the synonym out of the 5 alternatives for each verb and compute the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classic and google word vectors\n",
    "google_dict, classic_dict = defaultdict(int), defaultdict(int)\n",
    "\n",
    "with open('EN-wform.w.2.ppmi.svd.500.rcv_vocab.txt') as f:\n",
    "    classic_list = f.read().splitlines()\n",
    "    \n",
    "for index, vector in enumerate(classic_list):\n",
    "    word_vector = vector.split(' ')\n",
    "    classic_dict[word_vector[0]] = len(classic_dict)\n",
    "    classic_list[index] = word_vector[1:]\n",
    "\n",
    "with open('GoogleNews-vectors-rcv_vocab.txt') as f:\n",
    "    google_list = f.read().splitlines()\n",
    "\n",
    "for index, vector in enumerate(google_list):\n",
    "    word_vector = vector.split(' ')\n",
    "    google_dict[word_vector[0]] = len(google_dict)\n",
    "    google_list[index] = word_vector[1:]\n",
    "\n",
    "# Transform lists to matrices\n",
    "classic_matrix = np.matrix(classic_list).astype('float')\n",
    "google_matrix = np.matrix(google_list).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define google matrix and classic matrix\n",
    "google = DSWC(google_dict, google_matrix)\n",
    "classic = DSWC(classic_dict, classic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import synonym questions\n",
    "questions = json.load(open(\"synonyms_questions.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use google matrix and classic matrix to answer a synonym for each question\n",
    "accuracy_synonyms = [0] * 4\n",
    "\n",
    "for index, values in questions.items():   \n",
    "    sim_google = google.cosine_similarity(values['question'], values['choice'])\n",
    "    sim_classic = classic.cosine_similarity(values['question'], values['choice'])\n",
    "    dist_google = google.euclidean(values['question'], values['choice'])\n",
    "    dist_classic = classic.euclidean(values['question'], values['choice'])\n",
    "    ans_list = [sim_google.argmax(), sim_classic.argmax(), dist_classic.argmin(), dist_classic.argmin()]\n",
    "    for index, ans in enumerate(ans_list):\n",
    "        if ans == 0:\n",
    "            accuracy_synonyms[index] += 1\n",
    "            \n",
    "synonyms_detections =  pd.DataFrame(accuracy_synonyms, columns=['Accuracy'],\n",
    "                                    index=['cos_google', 'cos_classic', 'dist_google', 'dist_classic'])/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cos_google</th>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cos_classic</th>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dist_google</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dist_classic</th>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Accuracy\n",
       "cos_google       0.627\n",
       "cos_classic      0.537\n",
       "dist_google      1.000\n",
       "dist_classic     1.000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, Euclidean distance performs better in synonyms detections than cosine\n",
    "similarity. The former can correctly answer the question and gain 100%\n",
    "accuracy. As for COMPOSES and word2vec, if we look at the performance of cosine\n",
    "similarity, the Google matrix(word2vec) seems to have relatively better performance than\n",
    "the classic matrix(COMPOSES). (Accuracy: 62.7% > 53.7%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. SAT Analogy Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Objective: Use word vectors to solve 374 SAT Analogy Questions\n",
    "\n",
    "* Sample Question:\n",
    "\n",
    "MEDICINE : ILLNESS ::\n",
    "(a) law : anarchy\n",
    "(b) hunger : thirst\n",
    "(c) etiquette : discipline\n",
    "(d) love : treason\n",
    "(e) stimulant : sensitivity\n",
    "\n",
    "Correct Answer: (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SAT questions\n",
    "answer_dict = {answer: index for index, answer in enumerate(['a','b','c','d','e'])}\n",
    "sat_questions = defaultdict(dict)\n",
    "with open('SAT-package-V3.txt') as f:\n",
    "    sat = f.readlines()[42:]\n",
    "    \n",
    "for i in range(1,3364,9):\n",
    "    index = len(sat_questions)\n",
    "    sat_questions[index]['question'] = re.findall(r'[^\\W]+', sat[i])[:2]\n",
    "    choice_list = []\n",
    "    for choice in sat[(i+1):(i+6)]:\n",
    "        choice_list.append(re.findall(r'[^\\W]+',  choice)[:2])\n",
    "    sat_questions[index]['choice'] = choice_list\n",
    "    sat_questions[index]['answer'] = answer_dict[sat[i+6][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to summarize the relation between two words\n",
    "def vec_summary(v1, v2, mode):\n",
    "    if mode == 0:\n",
    "        vec = v1+v2\n",
    "    elif mode == 1:\n",
    "        vec = v1-v2\n",
    "    elif mode == 2:\n",
    "        vec = np.multiply(v1,v2)\n",
    "    elif mode == 3:\n",
    "        vec = np.multiply(v1,1/v2)\n",
    "    elif mode == 4:\n",
    "        vec = np.concatenate((v1,v2),axis=1)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wanyu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: RuntimeWarning: divide by zero encountered in true_divide\n",
      "C:\\Users\\wanyu\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:108: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "# Use cosine similarity to measure similarity\n",
    "results = {'Classic':[], 'Google':[]}\n",
    "for m in range(5):\n",
    "    accuracy_list_google = np.array([])\n",
    "    accuracy_list_classic = np.array([])\n",
    "\n",
    "    for index, values in sat_questions.items():\n",
    "        w1, w2 = values['question']\n",
    "        q1 = vec_summary(classic.word_vector(w1),classic.word_vector(w2),m)\n",
    "        q2 = vec_summary(google.word_vector(w1),google.word_vector(w2),m)\n",
    "        sim_list_classic, sim_list_google = np.array([]), np.array([])\n",
    "        for choice in values['choice']:\n",
    "            w1, w2 = choice\n",
    "            c1 = vec_summary(classic.word_vector(w1), classic.word_vector(w2),m)\n",
    "            c2 = vec_summary(google.word_vector(w1), google.word_vector(w2),m)\n",
    "            sim_list_classic = np.append(sim_list_classic, classic.cosine_similarity(q1,c1,v_input=True))\n",
    "            sim_list_google = np.append(sim_list_google, google.cosine_similarity(q2,c2,v_input=True))\n",
    "\n",
    "\n",
    "        accuracy_list_classic= np.append(accuracy_list_classic, sim_list_classic.argmax() == values['answer'])\n",
    "        accuracy_list_google= np.append(accuracy_list_classic, sim_list_google.argmax() == values['answer'])\n",
    "\n",
    "\n",
    "    results['Classic'].append(accuracy_list_classic.mean().round(4))\n",
    "    results['Google'].append(accuracy_list_google.mean().round(4))\n",
    "\n",
    "sat_cos = pd.DataFrame(results, index=['Addition', 'Subtraction', 'Multiplication', 'Division', 'Concatenation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Euclidean Distance to measure similarity\n",
    "results = {'Classic':[], 'Google':[]}\n",
    "for m in range(5):\n",
    "    accuracy_list_google = np.array([])\n",
    "    accuracy_list_classic = np.array([])\n",
    "\n",
    "    for index, values in sat_questions.items():\n",
    "        w1, w2 = values['question']\n",
    "        q1 = vec_summary(classic.word_vector(w1),classic.word_vector(w2),m)\n",
    "        q2 = vec_summary(google.word_vector(w1),google.word_vector(w2),m)\n",
    "        sim_list_classic, sim_list_google = np.array([]), np.array([])\n",
    "        for choice in values['choice']:\n",
    "            w1, w2 = choice\n",
    "            c1 = vec_summary(classic.word_vector(w1), classic.word_vector(w2),m)\n",
    "            c2 = vec_summary(google.word_vector(w1), google.word_vector(w2),m)\n",
    "            sim_list_classic = np.append(sim_list_classic, classic.euclidean(q1,c1,v_input=True))\n",
    "            sim_list_google = np.append(sim_list_google, google.euclidean(q2,c2,v_input=True))\n",
    "\n",
    "\n",
    "        accuracy_list_classic= np.append(accuracy_list_classic, sim_list_classic.argmin() == values['answer'])\n",
    "        accuracy_list_google= np.append(accuracy_list_classic, sim_list_google.argmin() == values['answer'])\n",
    "\n",
    "\n",
    "    results['Classic'].append(accuracy_list_classic.mean().round(4))\n",
    "    results['Google'].append(accuracy_list_google.mean().round(4))\n",
    "sat_dist = pd.DataFrame(results, index=['Addition', 'Subtraction', 'Multiplication', 'Division', 'Concatenation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classic</th>\n",
       "      <th>Google</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Addition</th>\n",
       "      <td>0.3102</td>\n",
       "      <td>0.3120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subtraction</th>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multiplication</th>\n",
       "      <td>0.2594</td>\n",
       "      <td>0.2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Division</th>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Concatenation</th>\n",
       "      <td>0.3904</td>\n",
       "      <td>0.3920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Classic  Google\n",
       "Addition         0.3102  0.3120\n",
       "Subtraction      0.3824  0.3840\n",
       "Multiplication   0.2594  0.2587\n",
       "Division         0.2032  0.2053\n",
       "Concatenation    0.3904  0.3920"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Aggregation Method</th>\n",
       "      <th>Division</th>\n",
       "      <th>Multiplication</th>\n",
       "      <th>Addition</th>\n",
       "      <th>Concatenation</th>\n",
       "      <th>Subtraction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Similarity</th>\n",
       "      <th>Word Matrix</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Cosine Similarity</th>\n",
       "      <th>Classic</th>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>0.4225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>0.1813</td>\n",
       "      <td>0.2373</td>\n",
       "      <td>0.3253</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>0.4213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Euclidean Distance</th>\n",
       "      <th>Classic</th>\n",
       "      <td>0.2032</td>\n",
       "      <td>0.2594</td>\n",
       "      <td>0.3102</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>0.3824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Google</th>\n",
       "      <td>0.2053</td>\n",
       "      <td>0.2587</td>\n",
       "      <td>0.3120</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>0.3840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy                                        \\\n",
       "Aggregation Method             Division Multiplication Addition Concatenation   \n",
       "Similarity         Word Matrix                                                  \n",
       "Cosine Similarity  Classic       0.1818         0.2353   0.3235        0.3904   \n",
       "                   Google        0.1813         0.2373   0.3253        0.3920   \n",
       "Euclidean Distance Classic       0.2032         0.2594   0.3102        0.3904   \n",
       "                   Google        0.2053         0.2587   0.3120        0.3920   \n",
       "\n",
       "                                            \n",
       "Aggregation Method             Subtraction  \n",
       "Similarity         Word Matrix              \n",
       "Cosine Similarity  Classic          0.4225  \n",
       "                   Google           0.4213  \n",
       "Euclidean Distance Classic          0.3824  \n",
       "                   Google           0.3840  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn the results into a table and compare them\n",
    "sat_dist['Similarity'] = 'Euclidean Distance'\n",
    "sat_cos['Similarity'] = 'Cosine Similarity'\n",
    "sat_comparison = pd.concat([sat_dist, sat_cos]).reset_index().rename(columns={'index':'Aggregation Method'})\n",
    "\n",
    "sat_comparison = pd.melt(sat_comparison, id_vars=['Similarity','Aggregation Method'], value_vars=['Classic', 'Google'], var_name='Word Matrix', value_name='Accuracy')\n",
    "#sat_comparison.iloc[:, [2,3,0,1,4]]\n",
    "#sat_comparison.groupby(['method','word_matrix','index']).mean().unstack(level=2)\n",
    "sat_comparison = pd.pivot_table(sat_comparison, index=['Similarity','Word Matrix'], columns='Aggregation Method', aggfunc=np.mean)\n",
    "sat_comparison.iloc[:,[2,3,0,1,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "To obtain a relation between two words within a question and its choices, I tried several methods, including division, multiplication, addition, concatenation, and subtraction. \n",
    "\n",
    "Subsequently, I compared those relations through computing their cosine similarity and\n",
    "Euclidean distance and chose the maximum value for cosine similarity and the minimum\n",
    "values for Euclidean distance. Besides, each method adopted two kinds of word\n",
    "matrices, the Classic matrix (COMPOSES) and the Google matrix (word2vect),\n",
    "respectively, to conduct computation. Finally, by calculating the accuracy for each\n",
    "method, the results are shown in the above table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clearly identify the best method, I visualized the results through the parallel coordinate plot and presented it in ​Figure 1​. Each vertical line represents the method for computing a relation between two words. Color lines exhibit the accuracy of different similarity methods. Red lines are cosine similarity whereas blue lines are Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "sat_plot = sat_comparison.reset_index()\n",
    "sat_plot['line_name'] = sat_plot.loc[:, ['method', 'level_1']].agg('_'.join, axis=1)\n",
    "sat_plot\n",
    "# Make the plot\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=[15,10]) \n",
    "p = parallel_coordinates(sat_plot.iloc[:,[4,5,2,3,6,7]], \n",
    "                         'line_name', \n",
    "                         color=['crimson', 'crimson', 'blue', 'blue'], #colormap=plt.get_cmap(\"Accent\")\n",
    "                         axvlines=False,\n",
    "                         linewidth=2.5)\n",
    "\n",
    "p.set_facecolor('white')\n",
    "p.spines['right'].set_color('0.5')\n",
    "p.spines['left'].set_color('0.5')\n",
    "for i in range(1,4,1):\n",
    "    plt.axvline(x=i, linewidth=0.5, color='black')\n",
    "plt.yticks(np.arange(0.2, 0.6, step=0.2))\n",
    "p.tick_params(axis='both', which='both', width=0, pad=5, color='white')\n",
    "plt.xticks(fontsize=16)\n",
    "p.set_yticklabels([str(i)+'%' for i in range(20,61,20)])\n",
    "plt.yticks(fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[0,'Substracting']+0.005, 'Cosine_Similarity', color='crimson', fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[0,'Substracting']-0.01, '(Google/Classic)', color='crimson', fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[3,'Substracting'], 'Euclidean Distance', color='blue', fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[3,'Substracting']-0.015, '(Google/Classic)', color='blue', fontsize=16)\n",
    "plt.legend().remove()\n",
    "plt.title('SAT Analogy Questions Accuracy', fontsize=26, y=1.05, color='#353333')\n",
    "plt.ylim(0,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "sat_plot = sat_comparison.stack(level=0).reset_index()\n",
    "sat_plot['line_name'] = sat_plot.loc[:, ['Similarity', 'Word Matrix']].agg('_'.join, axis=1)\n",
    "sat_plot.iloc[:,[5,6,3,4,7,8]]\n",
    "\n",
    "# Make the plot\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=[15,10]) \n",
    "p = parallel_coordinates(sat_plot.iloc[:,[5,6,3,4,7,8]], \n",
    "                         'line_name', \n",
    "                         color=['crimson', 'crimson', 'blue', 'blue'], #colormap=plt.get_cmap(\"Accent\")\n",
    "                         axvlines=False,\n",
    "                         linewidth=2.5)\n",
    "\n",
    "p.set_facecolor('white')\n",
    "p.spines['right'].set_color('0.5')\n",
    "p.spines['left'].set_color('0.5')\n",
    "for i in range(1,4,1):\n",
    "    plt.axvline(x=i, linewidth=0.5, color='black')\n",
    "plt.yticks(np.arange(0.2, 0.6, step=0.2))\n",
    "p.tick_params(axis='both', which='both', width=0, pad=5, color='white')\n",
    "plt.xticks(fontsize=16)\n",
    "p.set_yticklabels([str(i)+'%' for i in range(20,61,20)])\n",
    "plt.yticks(fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[0,'Subtraction']+0.005, 'Cosine_Similarity', color='crimson', fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[0,'Subtraction']-0.01, '(Google/Classic)', color='crimson', fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[3,'Subtraction'], 'Euclidean Distance', color='blue', fontsize=16)\n",
    "plt.text(4.03, sat_plot.loc[3,'Subtraction']-0.015, '(Google/Classic)', color='blue', fontsize=16)\n",
    "plt.legend().remove()\n",
    "plt.title('SAT Analogy Questions Accuracy', fontsize=26, y=1.05, color='#353333')\n",
    "plt.ylim(0,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that the best combination is to use the subtraction method and cosine\n",
    "similarity. For analogy questions, there is somehow a correlation between two words, but we might not be able to describe it as a positive or negative relationship. Take a\n",
    "question, \"ostrich\" and \" bird\" as an example. \"Ostrich\" is one kind of \"birds''. That is, \"ostrich\" belongs to the \"bird\" category. Therefore, the best answer is \"lion and cat\" because \"lion\" can also be classified into the \"cat\" category. The relationship between two words is that one word is a general concept, and the other word is a specific example of the concept. To capture this kind of relation, we can compute how much difference two word vectors have through the subtraction method. Besides, when using the subtraction method, the cosine similarity method has a better effect than Euclidean distance. Because subtraction summarizes a relation of two words into a vector and keeps a property of direction. The cosine similarity can recognize whether two vectors have a similar direction, but Euclidean distance only identifies the distance between two vectors. However, when using the concatenating method, because we kept all information from two words, Euclidean distance and cosine similarity had equivalent performances. Therefore, different aggregating methods should combine different similarity methods to optimize the performance\n",
    "\n",
    "As for the word matrices, the Classic matrix has similar performance with the Google\n",
    "matrix in no matter what method we use. For the best method ( subtraction and cosine\n",
    "similarity), the Classic matrix slightly better than the Google matrix (0.4225 > 0.4213).Overall, the numbers of entries in a vector will not cause a significant influence on\n",
    "accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
